{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phuocnguyen90/Random-projects/blob/main/WebUI_vi_gemma_2b_RAG_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please make sure you run this notebook with T4 runtime (change runtime type)\n",
        "\n",
        "Hãy chắc rằng bạn đang chạy notebook này với môi trường T4"
      ],
      "metadata": {
        "id": "i8-1h8ZHUod0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# disable cell output\n",
        "%%capture\n",
        "# update relevant packages\n",
        "!pip install transformers torch accelerate\n",
        "!pip install --upgrade gradio"
      ],
      "metadata": {
        "id": "HCwfY_eIC51V",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neZequ18CrGg"
      },
      "outputs": [],
      "source": [
        "# disable cell output\n",
        "%%capture\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# import relvant packages\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Khởi tạo tokenizer và model từ checkpoint đã lưu\n",
        "# Cell này sẽ mất khoảng 5-6 phút để tải và load mô hình từ huggingface\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"himmeow/vi-gemma-2b-RAG\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"himmeow/vi-gemma-2b-RAG\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Sử dụng GPU nếu có\n",
        "if torch.cuda.is_available():\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "from transformers import pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vi_gemma = pipeline(\n",
        "    \"document-question-answering\",  # RAG task\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "H4Uer59Ujrsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Định dạng prompt cho model\n",
        "PROMPT = \"\"\"\n",
        "### Instruction and Input:\n",
        "Dựa vào ngữ cảnh/tài liệu sau:\n",
        "{}\n",
        "Hãy trả lời câu hỏi: {}\n",
        "\n",
        "### Response:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "# Chuẩn bị dữ liệu đầu vào\n",
        "INPUT_DATA = \"\"\"\n",
        "3. Quy định về lập phương án và dự toán chi phí\n",
        "3.1. Nguyên tắc\n",
        "- Tất cả các khoản chi đều phải được phê duyệt chủ trương trước khi thực hiện.\n",
        "- Tất cả các khoản chi phải lập dự toán chi phí, kể cả chi phí lập, thẩm định, đánh giá hồ sơ, chi phí dự phòng (chi phí dự phòng trượt giá, chi phí dự phòng phát sinh khối lượng), phí, lệ phí và thuế.\n",
        "- Tất cả các khoản chi đều phải lập phương án thực hiện. Phương án ở đây có thể hiểu là phương án ngắn gọn (nội dung tối thiểu ở mục 3.3.1) được thể hiện trong cùng nội dung tờ trình chủ trương, hoặc phương án thực hiện thể hiện các nội dung thực hiện từng bước, được trình bày và phê duyệt trong một văn bản riêng, tùy theo mức độ phức tạp của việc thực hiện khoản chi.\n",
        "- Trên cơ sở phương án đã được phê duyệt, Đơn vị thực hiện tiến hành tìm kiếm hoặc lựa chọn nhà thầu/nhà cung cấp để thực hiện phương án theo quy trình được quy định tại Quy định này hoặc theo quy định của Luật đấu thầu.\n",
        "3.2. Đơn vị lập\n",
        "- Đơn vị thực hiện sẽ chủ trì lập phương án. Phương án cần được thẩm tra hoặc thẩm định trước khi trình cho Người có thẩm quyền phê duyệt phương án.\n",
        "- Các phương án phức tạp, nếu đơn vị thực hiện không đủ khả năng lập hồ sơ thì có thể thuê đơn vị tư vấn có đủ điều kiện, năng lực phù hợp lập phương án.\n",
        "3.3. Quy định về lập phương án và dự toán chi phí\n",
        "3.3.1. Đối với các phương án với hạn mức đến 100 triệu đồng\n",
        "Nội dung chủ trương bao gồm: Tên phương án/công việc; Căn cứ pháp lý; Đánh giá mức độ cần thiết; Các nội dung cần thực hiện; Thời gian thực hiện.\n",
        "Nội dung phương án tối thiểu phải bao gồm: Yêu cầu thực tế của việc xây dựng phương án; Các nội dung chi tiết cần thực hiện; Dự toán chi phí cho việc thực hiện phương án; Kế hoạch lựa chọn nhà thầu/nhà cung cấp; Hình thức quản lý và thực hiện.\n",
        "Căn cứ vào mức độ phức tạp của dự án, Người thẩm quyền có thể yêu cầu làm rõ thêm các nội dung phương án như liệt kê ở mục 3.3.2.\n",
        "3.3.2. Đối với các phương án với hạn mức trên 100 triệu đồng\n",
        "Nội dung của phương án bao gồm:\n",
        "- Sự cần thiết của phương án.\n",
        "- Mục tiêu.\n",
        "- Địa điểm trang bị/thực hiện/cải tạo.\n",
        "- Quy mô phương án, phạm vi cung cấp bao gồm chủng loại hàng hóa/loại hình dịch vụ và số lượng cung cấp.\n",
        "- Nguồn vốn\n",
        "- Dự toán chi phí thực hiện.\n",
        "- Tiêu chí kỹ thuật chi tiết.\n",
        "- Thời gian thực hiện hợp đồng, tiến độ cung cấp, triển khai.\n",
        "- Mẫu hàng hóa, sản phẩm (nếu có).\n",
        "- Tiến độ thực hiện.\n",
        "- Hiệu quả khi thực hiện phương án.\n",
        "- Kế hoạch lựa chọn nhà thầu/nhà cung cấp.\n",
        "- Các điều kiện cần thiết khác.\n",
        "- Hồ sơ phương án phải đóng thành quyển, có người lập và Trưởng đơn vị ký, đóng dấu (nếu có).\n",
        "3.3.3. Lưu ý: Trong phương án không nêu xuất xứ, nhãn hiệu của hàng hóa, trong trường hợp đặc biệt cần thiết phải nêu xuất xứ, nhãn hiệu của một nhà sản xuất nào đó, hoặc hàng hóa từ một nước nào đó để tham khảo, minh họa cho yêu cầu kỹ thuật của hàng hóa thì phải kèm theo cụm từ “hoặc tương đương” sau nhãn hiệu hoặc xuất xứ nêu ra và quy định rõ khái niệm tương đương nghĩa là có đặc tính kỹ thuật tương tự, có tính năng sử dụng là tương đương với các hàng hóa đã nêu.\n",
        "3.4. Căn cứ để lập dự toán\n",
        "- Quyết định giao kế hoạch chi phí sản xuất kinh doanh hàng năm/quý.\n",
        "- Giá dự toán chi phí: Thực hiện theo quy định tại Điều 16 Nghị định 24/2024/NĐ-CP. Một số điểm cần lưu ý khi thực hiện dự toán chi phí:\n",
        " Nếu công việc dự định thực hiện có định mức, đơn giá do Nhà nước công bố thì giá dự toán chi phí được hình thành trên cơ sở các đơn giá đó. Dự toán có thể được lập trên cơ sở đơn giá của kết quả đấu thầu/chào hàng cạnh tranh/chào hàng cạnh tranh gần nhất hoặc hợp đồng tương tự đã thực hiện trước đó.\n",
        " Nếu công việc dự định thực hiện đã có đơn giá trần do Tổng công ty quy định và ban hành thì giá dự toán chi phí được hình thành trên cơ sở không vượt đơn giá trần và có lợi nhất cho Công ty, Tổng công ty.\n",
        " Nếu công việc dự định thực hiện không có định mức, đơn giá do Nhà nước ban hành, không có đơn giá trần do Tổng công ty quy định thì giá dự toán chi phí phải được hình thành từ việc lập chi tiết từng nội dung kết hợp nghiên cứu khảo sát thực tế thị trường trên cơ sở tuân thủ theo quy định tại điểm b khoản 3 Điều 15, các điểm d, e, g khoản 2 Điều 16 Nghị định 24/2024/NĐ-CP. Theo đó, việc khảo sát giá cơ sở thông qua tối thiểu 01 báo giá của hàng hóa, dịch vụ, nhưng khuyến khích thu thập nhiều hơn 01 báo giá để có sự so sánh; trường hợp có nhiều hơn 01 báo giá thì lấy giá trung bình của các báo giá. Trong trường hợp không có nhiều nhà cung cấp trên địa bàn để so sánh thì có thể tham khảo trên địa bàn khác; hoặc từ các nguồn thông tin do cơ quan có thẩm quyền công bố, hoặc theo giá niêm yết của nhà sản xuất, nhà nhập khẩu, đại lý, nhà phân phối, nhà cung ứng, doanh nghiệp…\n",
        " Trường hợp chỉ định thầu, chỉ định nhà cung cấp: nếu công việc lập dự toán không có các định mức, đơn giá của Nhà nước, Tổng công ty ban hành thì việc lập dự toán dựa trên cơ sở:\n",
        " Kết quả đấu thầu, chào hàng cạnh tranh, hợp đồng tương tự gần nhất trước đó.\n",
        " Yêu cầu nhà cung cấp/nhà thầu được đề nghị chỉ định thực hiện phương án giải trình phương án xây dựng chi phí hợp lý đồng thời cung cấp hợp đồng tương tự đã thực hiện (nếu có).\n",
        " Nếu công việc phát sinh lần đầu (chưa có hợp đồng tương tự) thì đơn vị trình, đơn vị thẩm định và người có thẩm quyền phê duyệt phương án phải xem xét tính hợp lý của giá mà nhà cung cấp/nhà thầu được đề nghị chỉ định thực hiện phương án đưa ra và chịu trách nhiệm về quyết định của mình.\n",
        " Trường hợp gia hạn hợp đồng: dự toán, đơn giá không vượt hợp đồng đang thực hiện, tham khảo đơn giá các hợp đồng tương tự hoặc kết quả đấu thầu/chào hàng cạnh tranh gần nhất để đàm phán áp dụng đơn giá thấp nhất.\n",
        " Trường hợp mua sắm trực tiếp: căn cứ kết quả đấu thầu trước đó không quá 12 tháng kể từ ngày ký hợp đồng trước đó.\n",
        "3.5. Yêu cầu chung về tính hợp lệ của báo giá\n",
        "- Các báo giá đều phải được Người có thẩm quyền của đơn vị báo giá ký tên bằng chữ ký tươi, có đóng dấu và báo giá phải còn thời hạn hiệu lực.\n",
        "- Người trình phương án phải chịu trách nhiệm về tính pháp nhân, xác thực của các đơn vị cung cấp báo giá và tính hợp lý của báo giá, phương án trình phê duyệt.\n",
        "3.6. Kế hoạch lựa chọn nhà thầu/nhà cung cấp\n",
        "- Kế hoạch lựa chọn nhà thầu/nhà cung cấp được lập cho toàn bộ dự toán mua sắm hoặc một số gói thầu/gói mua sắm thuộc dự toán mua sắm để thực hiện trước. Trong kế hoạch lựa chọn nhà thầu/nhà cung cấp phải ghi rõ số lượng gói thầu/gói mua sắm và nội dung của từng gói thầu/gói mua sắm và việc áp dụng thỏa thuận khung (nếu có).\n",
        "- Việc phân chia dự toán mua sắm thành các gói thầu/gói mua sắm phải căn cứ theo tính chất kỹ thuật, trình tự thực hiện; đảm bảo tính đồng bộ và quy mô gói thầu/gói mua sắm hợp lý; không được phép chia nhỏ để áp dụng hình thức lựa chọn kém cạnh tranh hơn.\n",
        "- Nguồn chi phí thực hiện trong kế hoạch lựa chọn nhà thầu/nhà cung cấp:\n",
        " Căn cứ Quyết định giao kế hoạch chi phí sản xuất kinh doanh hàng năm/quý.\n",
        " Đối với các khoản chi cần thực hiện đấu thầu hoặc đấu thầu tập trung để ký hợp đồng khung áp dụng cho nhiều kỳ kế toán thì khi lập phương án không nhất thiết phải có kế hoạch chi phí cho những năm sau. Nguồn chi phí/vốn thực hiện ghi: Chi phí sản xuất kinh doanh của Công ty.\n",
        "- Chủ thể ký hợp đồng: Trong trường hợp Công ty đứng ra tổ chức lựa chọn nhà thầu/nhà cung cấp và đơn vị trực thuộc Công ty ký hợp đồng, chủ thể ký kết hợp đồng phải được xác định rõ trong quyết định phê duyệt phương án.\n",
        "3.7. Hồ sơ trình thẩm tra, thẩm định, phê duyệt phương án và dự toán chi phí\n",
        "3.6.1. Hồ sơ thẩm tra để phê duyệt phương án gồm:\n",
        "- Tờ trình phê duyệt phương án\n",
        "- Phương án thực hiện và dự toán chi phí.\n",
        "- Các hồ sơ khác có liên quan.\n",
        "Đơn vị thực hiện sẽ tiến hành tổ chức lập phương án và dự toán chi phí theo đúng các quy định hiện hành và chịu trách nhiệm trước Giám đốc Công ty và trước pháp luật về các hồ sơ này, trong trường hợp cần thuê đơn vị tư vấn bên ngoài để tiến hành các công việc trên cần phải báo cáo người có thẩm quyền xem xét chấp thuận.\n",
        "3.6.2. Hồ sơ thẩm định các bước theo quy định của Luật đấu thầu thì thực hiện theo quy định của pháp luật.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "qDuvp_TDDpL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Câu hỏi\n",
        "query = \"Các căn cứ nào để xác định giá dự toán chi phí?\""
      ],
      "metadata": {
        "id": "WEg9jEjIEAuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a response from the  model directly\n",
        "def get_vi_gemma_response(query):\n",
        "    input_text = PROMPT.format(INPUT_DATA, query,\" \")\n",
        "\n",
        "    # Mã hóa input text thành input ids\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Sử dụng GPU cho input ids nếu có\n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "    # Tạo văn bản bằng model\n",
        "    outputs = model.generate(\n",
        "        **input_ids,\n",
        "        max_new_tokens=500,\n",
        "        no_repeat_ngram_size=5,  # Ngăn chặn lặp lại các cụm từ 5 gram\n",
        "        #do_sample=True,   # Kích hoạt chế độ tạo văn bản dựa trên lấy mẫu. Trong chế độ này, model sẽ chọn ngẫu nhiên token tiếp theo dựa trên xác suất được tính từ phân phối xác suất của các token.\n",
        "        temperature=0.5,  # Giảm temperature để kiểm soát tính ngẫu nhiên\n",
        "        early_stopping=True,  # Dừng tạo văn bản khi tìm thấy kết thúc phù hợp\n",
        "    )\n",
        "    # Giải mã và xử lý kết quả để chỉ lấy phần bắt đầu với \"### Response:\"\n",
        "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Tìm phần bắt đầu với \"### Response:\" và cắt phần còn lại\n",
        "    response_start = decoded_output.find(\"### Response:\")\n",
        "    if response_start != -1:\n",
        "        response_text = decoded_output[response_start + len(\"### Response:\"):].strip()\n",
        "    else:\n",
        "        response_text = decoded_output\n",
        "    return response_text\n"
      ],
      "metadata": {
        "id": "c2MLCkPRhW0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Warning, the next cell would take  A LOT of time to generate an answer if not run on T4 GPU"
      ],
      "metadata": {
        "id": "JtpeMK5ycsPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chạy ô này để lấy kết quả câu hỏi\n",
        "response=get_vi_gemma_response(query)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "zbAFDfZ0n728",
        "outputId": "7dc4a9ad-cc40-40c4-f608-9d779455d17b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-49506ca47ac1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# chạy ô này để lấy kết quả câu hỏi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_vi_gemma_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-e9e482e7b846>\u001b[0m in \u001b[0;36mget_vi_gemma_response\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Tạo văn bản bằng model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m             \u001b[0;31m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1914\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   1915\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2652\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1128\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    929\u001b[0m                 )\n\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    932\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gemma/modeling_gemma.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nZrdS1rln_Qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phần này để kích hoạt giao diện người dùng"
      ],
      "metadata": {
        "id": "UMKc9MtwoBL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a response from the  model through pipeline\n",
        "# Formatting function for message and history\n",
        "def format_message(message: str, history: list, memory_limit: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Formats the message and history for the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        message (str): Current message to send.\n",
        "        history (list): Past conversation history.\n",
        "        memory_limit (int): Limit on how many past interactions to consider.\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted message string\n",
        "    \"\"\"\n",
        "    # always keep len(history) <= memory_limit\n",
        "    if len(history) > memory_limit:\n",
        "        history = history[-memory_limit:]\n",
        "\n",
        "    # Format the history part of the prompt\n",
        "    context = \"\"\n",
        "    for user_msg, model_answer in history:\n",
        "        context += f\"User: {user_msg}\\nModel: {model_answer}\\n\"\n",
        "\n",
        "    # Combine the context, current message, and prompt structure\n",
        "    formatted_message = PROMPT.format(context, message, \"\")\n",
        "    return formatted_message\n",
        "\n"
      ],
      "metadata": {
        "id": "NFMb7TsjmfeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gemma_response(message: str, input_data: str) -> str:\n",
        "    # Format the input text\n",
        "    input_text = PROMPT.format(input_data, message, \"\")\n",
        "\n",
        "    # Encode the input text\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Use GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "    # Generate text using the model\n",
        "    sequence = model.generate(\n",
        "        **input_ids,\n",
        "        max_new_tokens=500,\n",
        "        no_repeat_ngram_size=5,\n",
        "        temperature=0.5,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Decode the output and extract the response\n",
        "    decoded_output = tokenizer.decode(sequence[0], skip_special_tokens=True)\n",
        "    response_start = decoded_output.find(\"### Response:\")\n",
        "    if response_start != -1:\n",
        "        response_text = decoded_output[response_start + len(\"### Response:\"):].strip()\n",
        "    else:\n",
        "        response_text = decoded_output\n",
        "\n",
        "    return response_text"
      ],
      "metadata": {
        "id": "SIIKY_vB3AGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "# Gradio interface# Gradio interface\n",
        "def chat_interface(message, history):\n",
        "    response = get_gemma_response(message)\n",
        "    history.append([message, response])  # Ensure history is a list of lists\n",
        "    return history, history\n",
        "\n",
        "# Function to handle user message\n",
        "def user(message, history, input_data):\n",
        "    history.append([message, None])\n",
        "    return \"\", history, input_data\n",
        "\n",
        "# Function to handle bot response\n",
        "def chat_interface(history, input_data):\n",
        "    user_message = history[-1][0]\n",
        "    response = get_gemma_response(user_message, input_data)\n",
        "    history[-1][1] = response\n",
        "    return history\n",
        "\n",
        "# Function to enable input field\n",
        "def enable_input():\n",
        "    return gr.update(interactive=True)\n",
        "\n",
        "# Define Gradio blocks interface\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_data_box = gr.Textbox(label=\"Nội dung văn bản\", placeholder=\"Chép nội dung văn bản cần tìm kiếm vào đây\", lines=20, elem_id=\"input-data-box\")\n",
        "        with gr.Column():\n",
        "            chatbot = gr.Chatbot()\n",
        "            msg = gr.Textbox(label=\"Your Message\", placeholder=\"Enter your message here...\")\n",
        "            with gr.Row():\n",
        "                submit = gr.Button(\"Submit\")\n",
        "                clear = gr.Button(\"Clear\")\n",
        "\n",
        "    msg.submit(user, [msg, chatbot, input_data_box], [msg, chatbot, input_data_box], queue=False).then(\n",
        "        chat_interface, [chatbot, input_data_box], chatbot\n",
        "    ).then(\n",
        "        enable_input, None, [msg]\n",
        "    )\n",
        "    submit.click(user, [msg, chatbot, input_data_box], [msg, chatbot, input_data_box], queue=False).then(\n",
        "        chat_interface, [chatbot, input_data_box], chatbot\n",
        "    ).then(\n",
        "        enable_input, None, [msg]\n",
        "    )\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# Custom CSS to style the input data box\n",
        "demo.css = \"\"\"\n",
        "#input-data-box {\n",
        "    border: 2px solid #000;\n",
        "    padding: 10px;\n",
        "    border-radius: 5px;\n",
        "    background-color: #f9f9f9;\n",
        "    font-family: Arial, sans-serif;\n",
        "    font-size: 14px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "631YlnA5ifZ8",
        "outputId": "7d4512d5-e7da-40f1-a839-b03bb5e16b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://7eda8102887bbab313.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7eda8102887bbab313.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document chunking (work in progress)\n",
        "In this section we will chunk and embed longer document with the embedding model halong_embedding to be able to do RAG within a longer document"
      ],
      "metadata": {
        "id": "_fJBekPDfhPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_document(document, chunk_size=300):\n",
        "    \"\"\"\n",
        "    Splits the document into chunks of specified size.\n",
        "\n",
        "    Parameters:\n",
        "        document (str): The input document.\n",
        "        chunk_size (int): The size of each chunk.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of document chunks.\n",
        "    \"\"\"\n",
        "    words = document.split()\n",
        "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n"
      ],
      "metadata": {
        "id": "7Ow7lxQyfg35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install sentence_transformers\n",
        "!pip install rank_bm25"
      ],
      "metadata": {
        "id": "i6Urb1h1f_gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load the halong_embedding model\n",
        "embedding_model = SentenceTransformer(\"hiieu/halong_embedding\")\n",
        "\n",
        "def rank_chunks(query):\n",
        "    global context_chunks, context_embeddings\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    similarities = torch.tensor([embedding_model.similarity(query_embedding, [chunk_emb]).flatten() for chunk_emb in context_embeddings])\n",
        "    sorted_indices = torch.argsort(similarities, descending=True).flatten()\n",
        "    ranked_chunks = [context_chunks[idx] for idx in sorted_indices]\n",
        "    return ranked_chunks"
      ],
      "metadata": {
        "id": "QW3zcrxlf7s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_context(document, query, top_k=5):\n",
        "    \"\"\"\n",
        "    Retrieves the most relevant context for the query from the document.\n",
        "\n",
        "    Parameters:\n",
        "        document (str): The input document.\n",
        "        query (str): The user's query.\n",
        "        top_k (int): The number of top relevant chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        str: The relevant context.\n",
        "    \"\"\"\n",
        "    chunks = chunk_document(document)\n",
        "    ranked_chunks = rank_chunks(query, chunks)\n",
        "    return ' '.join(ranked_chunks[:top_k])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_R3o5P4hgcmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_context(document, query, top_k=5):\n",
        "    \"\"\"\n",
        "    Retrieves the most relevant context for the query from the document.\n",
        "\n",
        "    Parameters:\n",
        "        document (str): The input document.\n",
        "        query (str): The user's query.\n",
        "        top_k (int): The number of top relevant chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        str: The relevant context.\n",
        "    \"\"\"\n",
        "    chunks = chunk_document(document)\n",
        "    ranked_chunks = rank_chunks(chunks, query)\n",
        "    return ' '.join(ranked_chunks[:top_k])\n",
        "\n",
        "def chunk_and_embed_context(input_data):\n",
        "    global context_chunks, context_embeddings\n",
        "    context_chunks = chunk_document(input_data)\n",
        "    context_embeddings = embedding_model.encode(context_chunks)\n",
        "    return \"Context has been chunked and embedded.\"\n"
      ],
      "metadata": {
        "id": "ntwSh3TrgnnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_and_embed_context(INPUT_DATA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pka8hsWojdyE",
        "outputId": "e042bb01-3c40-4f44-94a3-3a4407eade54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Context has been chunked and embedded.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate the response using the chatbot model\n",
        "def get_gemma_response(message: str) -> str:\n",
        "    context = get_relevant_context(message)\n",
        "    input_text = PROMPT.format(context, message, \"\")\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = input_ids.to(\"cuda\")\n",
        "    sequence = model.generate(\n",
        "        **input_ids,\n",
        "        max_new_tokens=500,\n",
        "        no_repeat_ngram_size=5,\n",
        "        temperature=0.5,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    decoded_output = tokenizer.decode(sequence[0], skip_special_tokens=True)\n",
        "    response_start = decoded_output.find(\"### Response:\")\n",
        "    if response_start != -1:\n",
        "        response_text = decoded_output[response_start + len(\"### Response:\"):].strip()\n",
        "    else:\n",
        "        response_text = decoded_output\n",
        "    return response_text\n"
      ],
      "metadata": {
        "id": "wQ31EE4DgrrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define Gradio blocks interface\n",
        "\n",
        "# Define Gradio blocks interface\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_data_box = gr.Textbox(label=\"Context or Document\", placeholder=\"Enter your context or document here...\", lines=20, elem_id=\"input-data-box\")\n",
        "            chunk_button = gr.Button(\"Chunk and Embed Context\")\n",
        "        with gr.Column():\n",
        "            chatbot = gr.Chatbot()\n",
        "            msg = gr.Textbox(label=\"Your Message\", placeholder=\"Enter your message here...\")\n",
        "            with gr.Row():\n",
        "                submit = gr.Button(\"Submit\")\n",
        "                clear = gr.Button(\"Clear\")\n",
        "\n",
        "    chunk_button.click(chunk_and_embed_context, input_data_box, None)\n",
        "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
        "        chat_interface, chatbot, chatbot\n",
        "    ).then(\n",
        "        enable_input, None, [msg]\n",
        "    )\n",
        "    submit.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
        "        chat_interface, chatbot, chatbot\n",
        "    ).then(\n",
        "        enable_input, None, [msg]\n",
        "    )\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# Custom CSS to style the input data box\n",
        "demo.css = \"\"\"\n",
        "#input-data-box {\n",
        "    border: 2px solid #000;\n",
        "    padding: 10px;\n",
        "    border-radius: 5px;\n",
        "    background-color: #f9f9f9;\n",
        "    font-family: Arial, sans-serif;\n",
        "    font-size: 14px;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "4ffKkz-xgtPK",
        "outputId": "bd45fe2b-f086-4640-cd99-78a21c1c82b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://793178710292194533.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://793178710292194533.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}